- Porównanie do zaszumionego obrazka czy nie?
- ??
- Inne metody optymalizacji (SGD, momentum, nesterov)


- label propagation
- Semi-Supervised Learning via Compact Latent Space Clustering
- Po punktach z batcha H(y, x)
- DKL pomiędzy gaussami (analitycznie, każdy z każdym)
- Budowanie grafu
- alpha=1


98% najlepszy wynik (100d latent space, mała sieć konwolucyjna)
94% najlepszy wynik bez regularyzacji

1) Spike w errorach - co się dzieje?

Ilu studentów/doktorantów? 
Jakich ludzi? 
Czy mają być nazwiskami we wniosku (czy konkursy)? 
Ludzie z zewnątrz czy z tych jednostek?
Stawki wynagrodzeń
Etaty, umowy o dzieło, urlop z uczelni trzeba?
Na co jeszcze pieniądze - konferencje, wyjazdy, sprzęt
Utworzenie wniosku?

DONE:
6) NA JUTRO: Smiles, samplowanie z MNIST-a, interpolacje z MNIST-a, nowy wykres
4) Wykresy treningowe (trening na całym zbiorze, nie tylko labelki), valid/test (koszt klasy pokazujemy, ale nie doliczamy do suma)
5) -log(min(D(y|x) + eps, 1)) jako koszt, żeby nie wrzucać w peaki - 0.05 orientacyjnie
2) Drugi DKL (polabelowane dane, ten sam koszt)
10) Oba koszty klasyfikacji
1) Wariancja na 1, większa odległość na trunreg (100 * wariancja) (dziesięć, sto)
3) Minimalizowanie na przemian
9) CWAE odległość pomiędzy gaussem a punktami

TODO: 

7) Daj znać w przyszłym tygodniu mailowo jaki jest status
8) Eksperyment: tylko CWAE + Recerr

CURRENT BEST:

RZECZY DO TESTOWANIA:
1) koszt z epsilonem
2) logreg vs trunreg
3) unsupervised cost na polabelowanych
4) Startowa odległość pomiędzy gaussami
5) logdkl
6) Wyłączanie kosztów po stu epokach
7) SGD

OBSERWACJE:
Wszystko działa równie dobrze...?
mnist400d_logdklreg_scc_dw2.0_kn25_hd500_ccep0.1 - puścić na 250 epok?
- Odległość pomiędzy wielowymiarowymi przykładami powinna być większa niż jedna wariancja
- Szybkość uczenia zależy od epsilona w klasyfikacji (im większym, tym wolniej) - FAŁSZ!
- Szybkość uczenia zależy od wielkości batcha (im mniejszy, tym szybciej)
- Szybkie rozszerzanie na początek => brak rozszerzania później > brak rozszerzania na początku => szybkie rozszerzanie później 


kernel num 25 => 28, alpha => ON

057124


TODO: 
1) Większe batche - wygląda na to, że z większymi batchami działa gorzej. Pomysł - różna wielkość batchów.
2) Etykietowane punkty niech nie wchodzą do DKL-a nigdy. Rec Err - obie wersje? Co jakiś czas? Losowo 1 punkt z 50?

3) Regularyzacja L2 na średnie
4) Poprawić wykresiki
5) Lepsza architektura
6) Kilka opcji 
7) Za dwa tygodnie sprawdzamy wszystko
8) Metryki trainowe są chyba popsute? (Nie sumują się dobrze)
9) Odległośc CW

PYTANIA:
- Jakie powinno być gamma?
- Jak inicjalizacja średnich wpływa na wyniki?
- Tylko polabelowany koszt, a dopiero potem reszta? - Zadziałało słabo w starej wersji. Można sprawdzić znowu.
- CCEPS?
- Dla mnist 60kn jest słabawo. Czy problemem jest za duże wejście czy za duże wyjście? Spróbuj pierwsza warstwa * 3 
- Może powinniśmy zrobić żeby polabelowane punkty były nieco ważniejsze niż reszta?


OBSERWACJE:
- Ustawienie najbardziej pełne potencjału na ten moment: 100bs, inicjalizacja -5, 5, odległość kwartatowa. Zobaczymy, czy da się lepiej
- Chyba lepsza odległość liniowa (nie kwadratowa). Stabilniejsza? Kwadratowe daje minimalnie lepsze wyniki, ale jest mniej stabilna.
- 400hd > 600hd?


WYKONANE EKSPERYMENTY:
1) Fully connected z PCA (beznadziejne wyniki)
2) Rozpychanie dopiero od piątej epoki (chyba słabo?)
3) w mianowniku phi 0.1 zamiast 1.0 (???)
4) Zamrażanie meansów na epokę (?!?!)


2018-10-04 

OBSERWACJE:
1) Z dekodrem, który kompletnie nie działa i tak osiągamy prawie 98% acc. - mnist300d_cwdist_dw1000.0_kn45_hd400_bs100_sw1.0_mnist_cwlogits_tloff
2) Co jakby wyłączyć kompletnie rec error? A jakby eksponencjalny rec err? Upośledzony dekoder?
3) Batch norm?
4) Generowanie z większą wariancją - liczymy wariancję z sampli i z tego sampujemy
5) Jak z tą gammą? Po batchu czy po cały train secie
6) Sprawdź nany dla małych inicjalizacji
7) ccep?


2018-10-09

1) Porównanie funkcji kosztu:
a) Logits + rozpychanie - osiągamy ok. 98.7%, ale jest niestabilne i się psuje (zbadać czemu)
b) Single CW dist - Gaussy wyglądają ładnie, ale wyniki słabsze (98.4%)
c) CW logits - Gaussy wyglądają ładnie, wyniki 98.7%, wszystko stabilnie
    - Wygląda na to, że bez przemnożenia o stałą nie działa
d) Multi CW logits - ???, bardzo powolne, pamięciożerne

2) Pomysły na trenowanie
a) Normalnie
b) Brak rozpychania w początkowej fazie
c) Tylko rec err w początkowej fazie
d) Tylko dkl + rec err w poczatkowej fazie, potem means only

3) Różne testowane architektury

4) Inne kwestie
a) Błąd modelu z ReLU poprawia wyniki
b) eps w softmaksie nie pomaga (sprawdzić dla mniejszych wartości, sprawdzić kod?)
c) Gamma?
d) Rózne testowane architektury


2018-11-02
x 2) Gamma
3) Perturbacje w latent space/innego rodzaju szumy/przejrzyj pracki

Co jeśli wiemy, jakie jest prawdopodobieństwo klas, ale nie znamy klas?

2018-11-08
1) Minimalizacja entropii (P(y|x) z gęstościami).
2) Sprawdzamy dla każdego Gaussa fit score
3) Przemek - metryki
4) Gradient clipping
5) Ważenie prawdopodobieństw zamiast średniej


2018-11-15
(w kolejności)
1) Testy aktualnego modelu na SVHN (ADGM na SVHN)
2) Erf
3) Erf bez mianownika

4) Najlepszą metodę sprawdzamy na GMM
..............
x) Sprawdź przenoszenie o wektor żeby zachować style. 
x + 1) Wrzucamy wszystkie polabelowane do latent space i liczymy średnie żeby zainicjalizować meansy


2018-11-28
1) Odległość w erfie trzeba dzielić przez liczbę wymiarów, inaczej jest równa zero. Mądrzejszy sposób normalizacji?


-----------------------------------------------

EKSPERYMENT: jak waga log_cw wpływa na FID? Done - im większy logcw wym większy FID (robi sens)
EKSPERYMENT: jak ważenie supervised_weight wpłyawa na FID? - Done, im większe supervised tym gorszy FID
EKSPERYMENT: jak inicjalizajca wpływa na FID? norm vs onehot? - Onehot lepsze

PYTANIEten model zaczął działać dla takich ustawień jak w CWAE. Czy to batch_norm? Czy to dziwna architektura?

EKSPERYMENT: czy moja implementacja cpd jest taka sama jak Szymona?
EKSPERYMENT: cpd vs cw na jednym gaussie

EKSPERYMENT: cpd vs cw na gmm
EKSPERYMENT: batch_norm trainable=False

OBSERWACJA: batch_norm w latent space bardzo niestabilny. Przetstować na większym modelu (np. 64d)
